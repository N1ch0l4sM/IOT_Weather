{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round\n",
    "import config\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_postgres_health():\n",
    "    \"\"\"Check PostgreSQL database health\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=config.postG_db,\n",
    "            user=config.user,\n",
    "            password=config.password,\n",
    "            host=config.postG_host,\n",
    "            port=config.postG_port\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"ANALYZE;\")\n",
    "        \n",
    "        # Check database size\n",
    "        cursor.execute(\"SELECT pg_size_pretty(pg_database_size(current_database()))\")\n",
    "        db_size = cursor.fetchone()[0]\n",
    "        \n",
    "        # Check table sizes and counts\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                relname as table_name,\n",
    "                pg_size_pretty(pg_total_relation_size(relid)) as table_size,\n",
    "                n_live_tup as row_count\n",
    "            FROM pg_stat_user_tables\n",
    "            ORDER BY pg_total_relation_size(relid) DESC;\n",
    "        \"\"\")\n",
    "        table_stats = cursor.fetchall()\n",
    "        \n",
    "        # Log results\n",
    "        with open('/home/nicholas/Documents/IOT_Weather/db_logs/db_health.log', 'a') as f:\n",
    "            f.write(f\"\\n--- PostgreSQL Health Check ({datetime.now()}) ---\\n\")\n",
    "            f.write(f\"Database Size: {db_size}\\n\")\n",
    "            f.write(\"Table Statistics:\\n\")\n",
    "            for table in table_stats:\n",
    "                f.write(f\"  - {table[0]}: Size={table[1]}, Rows={table[2]}\\n\")\n",
    "            \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"PostgreSQL health check completed\"\n",
    "    except Exception as e:\n",
    "        return f\"PostgreSQL health check failed: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PostgreSQL health check completed'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_postgres_health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 15:52:45 WARN Utils: Your hostname, nicholas-MINI-S resolves to a loopback address: 127.0.1.1; using 192.168.0.245 instead (on interface wlo1)\n",
      "25/03/01 15:52:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/nicholas/miniconda3/envs/iot_weather/lib/python3.12/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/nicholas/.ivy2/cache\n",
      "The jars for the packages stored in: /home/nicholas/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-073da3d4-e1bb-4324-bd60-03b2e65c23f7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.6.0/postgresql-42.6.0.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.6.0!postgresql.jar (815ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.31.0/checker-qual-3.31.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.31.0!checker-qual.jar (179ms)\n",
      ":: resolution report :: resolve 2442ms :: artifacts dl 1002ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   2   |   2   |   0   ||   6   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-073da3d4-e1bb-4324-bd60-03b2e65c23f7\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 4 already retrieved (1274kB/8ms)\n",
      "25/03/01 15:52:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB to PostGres\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", f\"mongodb://{config.user}:{config.password}@{config.mongo_host}:{config.mongo_port}/{config.mongo_db}.iot_weather\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# MongoDB connection setup\n",
    "mongo_host = config.mongo_host\n",
    "mongo_port = config.mongo_port\n",
    "mongo_db = config.mongo_db\n",
    "mongo_collection = \"iot_weather\"\n",
    "\n",
    "# PostgreSQL connection properties\n",
    "postgres_properties = {\n",
    "    \"user\": config.user,\n",
    "    \"password\": config.password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(mongo_host, mongo_port, username=config.user, password=config.password)\n",
    "db = client[mongo_db]\n",
    "collection = db[mongo_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---------------+------------+--------+--------------+----------+-------+-----+\n",
      "|city_id|   city_name|avg_temperature|avg_humidity|avg_rain|avg_feels_like|avg_clouds|    lon|  lat|\n",
      "+-------+------------+---------------+------------+--------+--------------+----------+-------+-----+\n",
      "|     43|      Manama|         290.03|        62.0|     0.0|        289.39|       0.0|  50.58|26.23|\n",
      "|     22|    Tel Aviv|         287.18|       72.75|     0.0|        286.55|       0.0|  34.78|32.08|\n",
      "|     25|    Budapest|         279.26|       65.25|    0.03|        276.18|       0.0|  19.04|47.49|\n",
      "|     33|        Bern|         277.41|       73.25|     0.0|        272.85|       3.0|   2.17| 46.9|\n",
      "|     30|  Copenhagen|         276.72|        90.0|     0.0|        273.69|     72.75|  12.56|55.67|\n",
      "|     15|       Kabul|         287.21|       51.75|     0.0|        286.03|      20.0|  71.58|34.05|\n",
      "|     38|    Damascus|         283.82|        57.0|     0.0|        282.43|      6.25|  36.29|33.51|\n",
      "|     14|       Seoul|         277.28|       100.0|     0.0|         277.0|     81.25| 126.97|37.56|\n",
      "|     37|     Nicosia|         283.15|        86.0|     0.0|         282.2|      20.0|  33.37|35.22|\n",
      "|     27|        Oslo|         272.81|       86.75|     0.0|        272.81|     100.0|  10.75|59.91|\n",
      "|      9|   New Delhi|          293.2|        83.0|     0.0|        293.43|      20.0|   77.2|28.61|\n",
      "|     26|      Vienna|         277.06|       72.75|     0.0|        275.64|       0.0|  17.14|46.81|\n",
      "|     17| Mexico City|         297.38|        29.0|     0.0|        296.62|      5.33| -99.13|19.43|\n",
      "|      8|      Moscow|         274.21|       82.75|     0.0|        271.41|     100.0|  37.61|55.75|\n",
      "|     36|    Valletta|         288.18|        94.0|     0.0|        288.19|       0.0|  14.51|35.85|\n",
      "|     21|     Phoenix|         298.74|        11.5|     0.0|        298.18|      20.0|-112.07|33.44|\n",
      "|     11|Buenos Aires|         300.92|       65.25|     0.0|        302.82|      20.0| -58.38|-34.6|\n",
      "|     42| Kuwait City|          289.2|       36.75|     0.0|        287.82|     25.75|  47.97|29.37|\n",
      "|     18|      Warsaw|         274.98|        94.5|    0.27|        271.23|      75.0|  21.01|52.23|\n",
      "|     20|   Amsterdam|          277.3|        73.5|     0.0|        276.08|       5.0|   4.89|52.37|\n",
      "+-------+------------+---------------+------------+--------+--------------+----------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Successfully inserted 50 records into avg_hour table\n"
     ]
    }
   ],
   "source": [
    "# Calculate the date one hour ago from today\n",
    "one_hour_ago_timestamp = (datetime.now() - timedelta(hours=1)).timestamp()\n",
    "\n",
    "# Load data from MongoDB, filtering for data from one hour ago\n",
    "weather_data = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", f\"mongodb://{config.user}:{config.password}@{mongo_host}:{mongo_port}/{mongo_db}.{mongo_collection}?authSource=admin\") \\\n",
    "    .load() \\\n",
    "    .filter(col(\"dt\") >= one_hour_ago_timestamp)\n",
    "\n",
    "# Register the data frame as a temporary view for SQL queries\n",
    "weather_data.createOrReplaceTempView(\"weather_raw\")\n",
    "\n",
    "# SQL query to calculate averages with proper handling of null rain values\n",
    "avg_temps_sql = \"\"\"\n",
    "SELECT \n",
    "    ROUND(coord.lon, 2) AS lon, \n",
    "    ROUND(coord.lat, 2) AS lat,\n",
    "    ROUND(AVG(main.temp), 2) AS avg_temperature,\n",
    "    ROUND(AVG(main.humidity), 2) AS avg_humidity,\n",
    "    ROUND(AVG(COALESCE(rain.`1h`, 0)), 2) AS avg_rain,\n",
    "    ROUND(AVG(main.feels_like), 2) AS avg_feels_like,\n",
    "    ROUND(AVG(clouds.all), 2) AS avg_clouds\n",
    "FROM weather_raw\n",
    "GROUP BY ROUND(coord.lon, 2), ROUND(coord.lat, 2)\n",
    "\"\"\"\n",
    "\n",
    "avg_temps = spark.sql(avg_temps_sql)\n",
    "avg_temps.createOrReplaceTempView(\"avg_weather\")\n",
    "\n",
    "# Read city data directly from PostgreSQL using Spark JDBC\n",
    "city_df = spark.read \\\n",
    "    .jdbc(\n",
    "        url=f\"jdbc:postgresql://{config.postG_host}:{config.postG_port}/{config.postG_db}\",\n",
    "        table=\"city\",\n",
    "        properties=postgres_properties\n",
    "    )\n",
    "city_df.createOrReplaceTempView(\"cities\")\n",
    "\n",
    "# SQL join query to match city data with weather averages\n",
    "joined_sql = \"\"\"\n",
    "SELECT \n",
    "    c.idCity AS city_id,\n",
    "    c.CityName AS city_name,\n",
    "    w.avg_temperature,\n",
    "    w.avg_humidity,\n",
    "    w.avg_rain,\n",
    "    w.avg_feels_like,\n",
    "    w.avg_clouds,\n",
    "    w.lon,\n",
    "    w.lat\n",
    "FROM avg_weather w\n",
    "JOIN cities c \n",
    "    ON ROUND(w.lat, 2) = ROUND(c.lat, 2)\n",
    "    AND ROUND(w.lon, 2) = ROUND(c.lon, 2)\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(joined_sql)\n",
    "\n",
    "# Show the joined results\n",
    "result_df.show()\n",
    "\n",
    "# # Get current date and hour\n",
    "current_date = (datetime.now() - timedelta(hours=1)).strftime(\"%Y-%m-%d\")\n",
    "one_hour_ago_num = datetime.now().hour\n",
    "\n",
    "# Convert DataFrame to a list of tuples for batch insertion\n",
    "rows_to_insert = [(\n",
    "    row.city_id, \n",
    "    row.avg_temperature, \n",
    "    row.avg_rain, \n",
    "    row.avg_feels_like,\n",
    "    row.avg_clouds,\n",
    "    current_date, \n",
    "    one_hour_ago_num\n",
    ") for row in result_df.collect()]\n",
    "\n",
    "# Connect to PostgreSQL for insertion\n",
    "postgre_conn = psycopg2.connect(\n",
    "    dbname=config.postG_db,\n",
    "    user=config.user,\n",
    "    password=config.password,\n",
    "    host=config.postG_host,\n",
    "    port=config.postG_port\n",
    ")\n",
    "cursor = postgre_conn.cursor()\n",
    "\n",
    "# Insert data into PostgreSQL\n",
    "if rows_to_insert:\n",
    "    cursor.executemany(\"\"\"\n",
    "    INSERT INTO weather_hour (IdCity, Temp, Rain, Clouds, FeelsLike, Date, Hour)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", rows_to_insert)\n",
    "    postgre_conn.commit()\n",
    "    print(f\"Successfully inserted {len(rows_to_insert)} records into avg_hour table\")\n",
    "else:\n",
    "    print(\"No new data to insert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the PostgreSQL connection\n",
    "cursor.close()\n",
    "postgre_conn.close()\n",
    "\n",
    "# Close the MongoDB connection\n",
    "client.close()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot_weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
